{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fbc5ca-41b0-4c51-b880-8fa15ebbe313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99836ba5-a9d6-4f09-9fc6-31258efaf958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from mlflow.models.signature import infer_signature\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from src.data_utils import load_data, get_features, get_target, numerical_features, categorical_features\n",
    "# from src.utils import generate_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "# --- Placeholder implementations ---\n",
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    # Replace with your actual data loading logic\n",
    "    # For now, create a dummy DataFrame\n",
    "    data = {\n",
    "        'force_on_tire': np.random.rand(100),\n",
    "        'degradation_risk': np.random.choice(['safe', 'medium', 'critical'], 100),\n",
    "        'Track': np.random.choice(['Monza', 'Silverstone', 'Spa'], 100)\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def get_features(df):\n",
    "    # Dummy split: all numeric except 'degradation_risk' and 'Track'\n",
    "    num_cols = [col for col in df.columns if df[col].dtype in [np.float64, np.int64]]\n",
    "    cat_cols = [col for col in df.columns if df[col].dtype == object and col not in ['degradation_risk']]\n",
    "    return df[num_cols], df[cat_cols]\n",
    "\n",
    "def get_target(df):\n",
    "    return df['degradation_risk']\n",
    "\n",
    "numerical_features = ['force_on_tire']\n",
    "categorical_features = ['Track']\n",
    "\n",
    "def generate_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    return cm\n",
    "# --- End placeholders ---\n",
    "\n",
    "# MLflow setup (Databricks auto—generic experiment for portability)\n",
    "mlflow.set_experiment(\"/Users/jrcarra1@asu.edu/F1_Tire_Deg_DemoII\")\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Add lap_count if missing (simulates stint: F1 races ~50-70 laps, deg ramps exponentially)\n",
    "if 'lap_count' not in df.columns:\n",
    "    df['lap_count'] = np.random.randint(1, 71, len(df))  # Uniform; real telemetry sequential per stint\n",
    "mlflow.log_param(\"lap_count_simulated\", 'lap_count' not in df.columns)\n",
    "\n",
    "# Add interaction for cumulative deg (lap * force amps wear under repeated loads)\n",
    "df['lap_force'] = df['lap_count'] * df['force_on_tire']\n",
    "numerical_features += ['lap_count', 'lap_force']  # Now 16 num features—retrain fixes shape errors\n",
    "\n",
    "# Split (stratify by class+track for balance/gen—critical rare, avoids Monza bias)\n",
    "stratify_col = df['degradation_risk'].astype(str) + df['Track']\n",
    "train_idx, test_idx = train_test_split(df.index, test_size=0.2, random_state=42, stratify=stratify_col)\n",
    "train_df = df.loc[train_idx]\n",
    "test_df = df.loc[test_idx]\n",
    "\n",
    "# Preprocess (num/cat split, encode, scale)\n",
    "X_train_num, X_train_cat = get_features(train_df)\n",
    "X_test_num, X_test_cat = get_features(test_df)\n",
    "y_train = get_target(train_df)\n",
    "y_test = get_target(test_df)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2000130-852d-4e3a-81ec-dc123da601b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Training_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
